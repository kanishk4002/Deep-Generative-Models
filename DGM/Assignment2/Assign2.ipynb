{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "qKhX-_v7Re_s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code implements a Variational Autoencoder (VAE) using TensorFlow/Keras. It includes an encoder to compress data into a latent space and a decoder to reconstruct the original data from sampled latent vectors. The VAE allows for efficient representation learning and generation of data.\n"
      ],
      "metadata": {
        "id": "wTpr_GbJR5Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalAutoencoder(tf.keras.Model):\n",
        "    def __init__(self, input_shape, output_shape, latent_dim=32):\n",
        "        \"\"\"\n",
        "        Initialize the Variational Autoencoder (VAE).\n",
        "\n",
        "        Args:\n",
        "        - input_shape: Tuple, shape of the input data (height, width, channels).\n",
        "        - output_shape: Tuple, shape of the output data (height, width, channels).\n",
        "        - latent_dim: Integer, dimensionality of the latent space.\n",
        "        \"\"\"\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder architecture\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=input_shape),\n",
        "            tf.keras.layers.Conv2D(32, 3, strides=2, activation='relu'),\n",
        "            tf.keras.layers.Conv2D(64, 3, strides=2, activation='relu'),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(latent_dim * 2)\n",
        "        ])\n",
        "\n",
        "        # Decoder architecture\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "            tf.keras.layers.Dense(input_shape[0]//4 * input_shape[1]//4 * 64, activation='relu'),\n",
        "            tf.keras.layers.Reshape((input_shape[0]//4, input_shape[1]//4, 64)),\n",
        "            tf.keras.layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same'),\n",
        "            tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same'),\n",
        "            tf.keras.layers.Conv2D(output_shape[-1], 3, activation='sigmoid', padding='same')\n",
        "        ])\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        Encode input data to obtain mean and log-variance parameters of the latent distribution.\n",
        "\n",
        "        Args:\n",
        "        - x: Tensor, input data.\n",
        "\n",
        "        Returns:\n",
        "        - mean: Tensor, mean of the latent distribution.\n",
        "        - logvar: Tensor, log-variance of the latent distribution.\n",
        "        \"\"\"\n",
        "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "        return mean, logvar\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        \"\"\"\n",
        "        Reparameterization trick to sample from the latent distribution.\n",
        "\n",
        "        Args:\n",
        "        - mean: Tensor, mean of the latent distribution.\n",
        "        - logvar: Tensor, log-variance of the latent distribution.\n",
        "\n",
        "        Returns:\n",
        "        - z: Tensor, sampled latent vector.\n",
        "        \"\"\"\n",
        "        eps = tf.random.normal(shape=mean.shape)\n",
        "        return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"\n",
        "        Decode latent vector to reconstruct the input data.\n",
        "\n",
        "        Args:\n",
        "        - z: Tensor, latent vector.\n",
        "\n",
        "        Returns:\n",
        "        - reconstructed: Tensor, reconstructed data.\n",
        "        \"\"\"\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass through the VAE.\n",
        "\n",
        "        Args:\n",
        "        - inputs: Tensor, input data.\n",
        "\n",
        "        Returns:\n",
        "        - reconstructed: Tensor, reconstructed data.\n",
        "        - mean: Tensor, mean of the latent distribution.\n",
        "        - logvar: Tensor, log-variance of the latent distribution.\n",
        "        \"\"\"\n",
        "        mean, logvar = self.encode(inputs)\n",
        "        z = self.reparameterize(mean, logvar)\n",
        "        return self.decode(z), mean, logvar\n"
      ],
      "metadata": {
        "id": "BWseQaldR9Kq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoderVAE(VariationalAutoencoder):\n",
        "    def __init__(self, input_shape, output_shape, latent_dim=32):\n",
        "        \"\"\"\n",
        "        Initialize the Encoder-Decoder VAE.\n",
        "\n",
        "        Args:\n",
        "        - input_shape: Tuple, shape of the input data (height, width, channels).\n",
        "        - output_shape: Tuple, shape of the output data (height, width, channels).\n",
        "        - latent_dim: Integer, dimensionality of the latent space.\n",
        "        \"\"\"\n",
        "        super(EncoderDecoderVAE, self).__init__(input_shape, output_shape, latent_dim)\n",
        "        # Customize the encoder architecture if necessary\n",
        "\n",
        "class DecoderEncoderDecoderVAE(VariationalAutoencoder):\n",
        "    def __init__(self, input_shape, output_shape, latent_dim=32):\n",
        "        \"\"\"\n",
        "        Initialize the Decoder-Encoder-Decoder VAE.\n",
        "\n",
        "        Args:\n",
        "        - input_shape: Tuple, shape of the input data (height, width, channels).\n",
        "        - output_shape: Tuple, shape of the output data (height, width, channels).\n",
        "        - latent_dim: Integer, dimensionality of the latent space.\n",
        "        \"\"\"\n",
        "        super(DecoderEncoderDecoderVAE, self).__init__(input_shape, output_shape, latent_dim)\n",
        "        # Customize the decoder architecture if necessary\n",
        "\n",
        "class DoubleEncoderDecoderVAE(VariationalAutoencoder):\n",
        "    def __init__(self, input_shape, output_shape, latent_dim=32):\n",
        "        \"\"\"\n",
        "        Initialize the Double Encoder-Decoder VAE.\n",
        "\n",
        "        Args:\n",
        "        - input_shape: Tuple, shape of the input data (height, width, channels).\n",
        "        - output_shape: Tuple, shape of the output data (height, width, channels).\n",
        "        - latent_dim: Integer, dimensionality of the latent space.\n",
        "        \"\"\"\n",
        "        super(DoubleEncoderDecoderVAE, self).__init__(input_shape, output_shape, latent_dim)\n",
        "        # Customize both encoder and decoder architectures if necessary\n"
      ],
      "metadata": {
        "id": "IMqAvltFSHoi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess MNIST data\n",
        "def load_and_preprocess_data():\n",
        "    \"\"\"\n",
        "    Load and preprocess MNIST dataset.\n",
        "\n",
        "    Returns:\n",
        "    - x_train: Numpy array, preprocessed training data.\n",
        "    - x_test: Numpy array, preprocessed test data.\n",
        "    \"\"\"\n",
        "    (x_train, _), (x_test, _) = mnist.load_data()\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "    x_train = np.expand_dims(x_train, axis=-1)\n",
        "    x_test = np.expand_dims(x_test, axis=-1)\n",
        "    return x_train, x_test\n",
        "\n",
        "# Function to train and evaluate a VAE model\n",
        "def train_and_evaluate_vae(model, x_train, x_test, epochs=30, batch_size=128):\n",
        "    \"\"\"\n",
        "    Train and evaluate a VAE model.\n",
        "\n",
        "    Args:\n",
        "    - model: Instance of VAE model.\n",
        "    - x_train: Numpy array, preprocessed training data.\n",
        "    - x_test: Numpy array, preprocessed test data.\n",
        "    - epochs: Integer, number of training epochs.\n",
        "    - batch_size: Integer, batch size for training.\n",
        "\n",
        "    Returns:\n",
        "    - test_loss: Float, final test loss.\n",
        "    - test_losses: List of floats, test losses over epochs.\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(x):\n",
        "        with tf.GradientTape() as tape:\n",
        "            reconstruction, mean, logvar = model(x)\n",
        "            reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(x, reconstruction))\n",
        "            kl_loss = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return total_loss, reconstruction_loss, kl_loss\n",
        "\n",
        "    test_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss, epoch_reconstruction_loss, epoch_kl_loss = 0, 0, 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch in tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size):\n",
        "            total_loss, reconstruction_loss, kl_loss = train_step(batch)\n",
        "            epoch_loss += total_loss\n",
        "            epoch_reconstruction_loss += reconstruction_loss\n",
        "            epoch_kl_loss += kl_loss\n",
        "            num_batches += 1\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "        epoch_reconstruction_loss /= num_batches\n",
        "        epoch_kl_loss /= num_batches\n",
        "\n",
        "        test_reconstruction, _, _ = model(x_test)\n",
        "        test_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(x_test, test_reconstruction))\n",
        "        test_losses.append(test_loss.numpy())\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Train Loss: {epoch_loss:.4f}, Reconstruction Loss: {epoch_reconstruction_loss:.4f}, KL Loss: {epoch_kl_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
        "\n",
        "    return test_losses[-1], test_losses\n"
      ],
      "metadata": {
        "id": "Aoa-h4y8SOqi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot test losses for all models\n",
        "def plot_test_losses(model_losses):\n",
        "    \"\"\"\n",
        "    Plot test losses for all VAE models.\n",
        "\n",
        "    Args:\n",
        "    - model_losses: Dictionary, mapping model names to lists of test losses.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for model_name, losses in model_losses.items():\n",
        "        plt.plot(range(1, len(losses) + 1), losses, label=model_name)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Test Loss')\n",
        "    plt.title('Test Loss per Epoch for Different VAE Models')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('vae_models_test_losses.png')\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "pqpdi_LzmeDa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main script to load data, train models, and evaluate\n",
        "def main():\n",
        "    x_train, x_test = load_and_preprocess_data()\n",
        "\n",
        "    models = {\n",
        "        'VanillaVAE': VariationalAutoencoder(input_shape=(28, 28, 1), output_shape=(28, 28, 1)),\n",
        "        'EncoderDecoderVAE': EncoderDecoderVAE(input_shape=(28, 28, 1), output_shape=(28, 28, 1)),\n",
        "        'DecoderEncoderDecoderVAE': DecoderEncoderDecoderVAE(input_shape=(28, 28, 1), output_shape=(28, 28, 1)),\n",
        "        'DoubleEncoderDecoderVAE': DoubleEncoderDecoderVAE(input_shape=(28, 28, 1), output_shape=(28, 28, 1))\n",
        "    }\n",
        "\n",
        "    model_losses = {}\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nTraining {model_name}...\")\n",
        "        final_loss, epoch_losses = train_and_evaluate_vae(model, x_train, x_test)\n",
        "        model_losses[model_name] = epoch_losses\n",
        "        print(f'{model_name} Final Test Loss: {final_loss:.4f}')\n",
        "\n",
        "    plot_test_losses(model_losses)\n",
        "\n",
        "    # Find the best performing model\n",
        "    best_model = min(model_losses, key=lambda x: model_losses[x][-1])\n",
        "    print(f\"\\nBest performing model: {best_model}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PoaJ6dgmlmK",
        "outputId": "d4453f01-ce5f-48ae-dd1c-f40312d095cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training VanillaVAE...\n",
            "Epoch 1, Train Loss: 0.3839, Reconstruction Loss: 0.3815, KL Loss: 0.0024, Test Loss: 0.2744\n",
            "Epoch 2, Train Loss: 0.2704, Reconstruction Loss: 0.2687, KL Loss: 0.0018, Test Loss: 0.2662\n",
            "Epoch 3, Train Loss: 0.2673, Reconstruction Loss: 0.2656, KL Loss: 0.0018, Test Loss: 0.2644\n",
            "Epoch 4, Train Loss: 0.2660, Reconstruction Loss: 0.2643, KL Loss: 0.0018, Test Loss: 0.2633\n",
            "Epoch 5, Train Loss: 0.2651, Reconstruction Loss: 0.2634, KL Loss: 0.0018, Test Loss: 0.2625\n",
            "Epoch 6, Train Loss: 0.2646, Reconstruction Loss: 0.2628, KL Loss: 0.0018, Test Loss: 0.2623\n",
            "Epoch 7, Train Loss: 0.2643, Reconstruction Loss: 0.2625, KL Loss: 0.0018, Test Loss: 0.2619\n",
            "Epoch 8, Train Loss: 0.2641, Reconstruction Loss: 0.2622, KL Loss: 0.0019, Test Loss: 0.2616\n",
            "Epoch 9, Train Loss: 0.2640, Reconstruction Loss: 0.2621, KL Loss: 0.0019, Test Loss: 0.2616\n",
            "Epoch 10, Train Loss: 0.2637, Reconstruction Loss: 0.2617, KL Loss: 0.0020, Test Loss: 0.2613\n",
            "Epoch 11, Train Loss: 0.2637, Reconstruction Loss: 0.2617, KL Loss: 0.0020, Test Loss: 0.2612\n",
            "Epoch 12, Train Loss: 0.2636, Reconstruction Loss: 0.2616, KL Loss: 0.0020, Test Loss: 0.2608\n",
            "Epoch 13, Train Loss: 0.2635, Reconstruction Loss: 0.2614, KL Loss: 0.0021, Test Loss: 0.2610\n",
            "Epoch 14, Train Loss: 0.2635, Reconstruction Loss: 0.2614, KL Loss: 0.0021, Test Loss: 0.2608\n",
            "Epoch 15, Train Loss: 0.2633, Reconstruction Loss: 0.2612, KL Loss: 0.0021, Test Loss: 0.2609\n",
            "Epoch 16, Train Loss: 0.2634, Reconstruction Loss: 0.2613, KL Loss: 0.0021, Test Loss: 0.2606\n",
            "Epoch 17, Train Loss: 0.2633, Reconstruction Loss: 0.2611, KL Loss: 0.0021, Test Loss: 0.2607\n",
            "Epoch 18, Train Loss: 0.2633, Reconstruction Loss: 0.2611, KL Loss: 0.0021, Test Loss: 0.2609\n",
            "Epoch 19, Train Loss: 0.2633, Reconstruction Loss: 0.2611, KL Loss: 0.0021, Test Loss: 0.2605\n",
            "Epoch 20, Train Loss: 0.2631, Reconstruction Loss: 0.2610, KL Loss: 0.0022, Test Loss: 0.2604\n",
            "Epoch 21, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0022, Test Loss: 0.2605\n",
            "Epoch 22, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2604\n",
            "Epoch 23, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2605\n",
            "Epoch 24, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2605\n",
            "Epoch 25, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2604\n",
            "Epoch 26, Train Loss: 0.2630, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2603\n",
            "Epoch 27, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2604\n",
            "Epoch 28, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0022, Test Loss: 0.2601\n",
            "Epoch 29, Train Loss: 0.2630, Reconstruction Loss: 0.2607, KL Loss: 0.0023, Test Loss: 0.2601\n",
            "Epoch 30, Train Loss: 0.2629, Reconstruction Loss: 0.2606, KL Loss: 0.0023, Test Loss: 0.2601\n",
            "VanillaVAE Final Test Loss: 0.2601\n",
            "\n",
            "Training EncoderDecoderVAE...\n",
            "Epoch 1, Train Loss: 0.3875, Reconstruction Loss: 0.3856, KL Loss: 0.0019, Test Loss: 0.2739\n",
            "Epoch 2, Train Loss: 0.2702, Reconstruction Loss: 0.2687, KL Loss: 0.0016, Test Loss: 0.2664\n",
            "Epoch 3, Train Loss: 0.2672, Reconstruction Loss: 0.2655, KL Loss: 0.0017, Test Loss: 0.2645\n",
            "Epoch 4, Train Loss: 0.2659, Reconstruction Loss: 0.2642, KL Loss: 0.0017, Test Loss: 0.2635\n",
            "Epoch 5, Train Loss: 0.2650, Reconstruction Loss: 0.2633, KL Loss: 0.0018, Test Loss: 0.2626\n",
            "Epoch 6, Train Loss: 0.2647, Reconstruction Loss: 0.2629, KL Loss: 0.0018, Test Loss: 0.2622\n",
            "Epoch 7, Train Loss: 0.2644, Reconstruction Loss: 0.2626, KL Loss: 0.0018, Test Loss: 0.2620\n",
            "Epoch 8, Train Loss: 0.2640, Reconstruction Loss: 0.2623, KL Loss: 0.0018, Test Loss: 0.2619\n",
            "Epoch 9, Train Loss: 0.2639, Reconstruction Loss: 0.2620, KL Loss: 0.0019, Test Loss: 0.2614\n",
            "Epoch 10, Train Loss: 0.2638, Reconstruction Loss: 0.2620, KL Loss: 0.0019, Test Loss: 0.2614\n",
            "Epoch 11, Train Loss: 0.2637, Reconstruction Loss: 0.2618, KL Loss: 0.0019, Test Loss: 0.2613\n",
            "Epoch 12, Train Loss: 0.2636, Reconstruction Loss: 0.2616, KL Loss: 0.0019, Test Loss: 0.2612\n",
            "Epoch 13, Train Loss: 0.2635, Reconstruction Loss: 0.2615, KL Loss: 0.0020, Test Loss: 0.2611\n",
            "Epoch 14, Train Loss: 0.2633, Reconstruction Loss: 0.2613, KL Loss: 0.0020, Test Loss: 0.2608\n",
            "Epoch 15, Train Loss: 0.2634, Reconstruction Loss: 0.2612, KL Loss: 0.0021, Test Loss: 0.2610\n",
            "Epoch 16, Train Loss: 0.2633, Reconstruction Loss: 0.2612, KL Loss: 0.0021, Test Loss: 0.2609\n",
            "Epoch 17, Train Loss: 0.2633, Reconstruction Loss: 0.2612, KL Loss: 0.0021, Test Loss: 0.2608\n",
            "Epoch 18, Train Loss: 0.2633, Reconstruction Loss: 0.2612, KL Loss: 0.0021, Test Loss: 0.2606\n",
            "Epoch 19, Train Loss: 0.2632, Reconstruction Loss: 0.2611, KL Loss: 0.0021, Test Loss: 0.2608\n",
            "Epoch 20, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0022, Test Loss: 0.2606\n",
            "Epoch 21, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0023, Test Loss: 0.2603\n",
            "Epoch 22, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0022, Test Loss: 0.2605\n",
            "Epoch 23, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0022, Test Loss: 0.2606\n",
            "Epoch 24, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0022, Test Loss: 0.2603\n",
            "Epoch 25, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0022, Test Loss: 0.2603\n",
            "Epoch 26, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0022, Test Loss: 0.2603\n",
            "Epoch 27, Train Loss: 0.2630, Reconstruction Loss: 0.2608, KL Loss: 0.0022, Test Loss: 0.2604\n",
            "Epoch 28, Train Loss: 0.2630, Reconstruction Loss: 0.2608, KL Loss: 0.0022, Test Loss: 0.2605\n",
            "Epoch 29, Train Loss: 0.2629, Reconstruction Loss: 0.2606, KL Loss: 0.0023, Test Loss: 0.2602\n",
            "Epoch 30, Train Loss: 0.2630, Reconstruction Loss: 0.2607, KL Loss: 0.0023, Test Loss: 0.2602\n",
            "EncoderDecoderVAE Final Test Loss: 0.2602\n",
            "\n",
            "Training DecoderEncoderDecoderVAE...\n",
            "Epoch 1, Train Loss: 0.3847, Reconstruction Loss: 0.3824, KL Loss: 0.0023, Test Loss: 0.2739\n",
            "Epoch 2, Train Loss: 0.2703, Reconstruction Loss: 0.2687, KL Loss: 0.0016, Test Loss: 0.2660\n",
            "Epoch 3, Train Loss: 0.2673, Reconstruction Loss: 0.2657, KL Loss: 0.0016, Test Loss: 0.2647\n",
            "Epoch 4, Train Loss: 0.2662, Reconstruction Loss: 0.2644, KL Loss: 0.0017, Test Loss: 0.2635\n",
            "Epoch 5, Train Loss: 0.2652, Reconstruction Loss: 0.2634, KL Loss: 0.0018, Test Loss: 0.2628\n",
            "Epoch 6, Train Loss: 0.2647, Reconstruction Loss: 0.2628, KL Loss: 0.0018, Test Loss: 0.2622\n",
            "Epoch 7, Train Loss: 0.2642, Reconstruction Loss: 0.2623, KL Loss: 0.0019, Test Loss: 0.2621\n",
            "Epoch 8, Train Loss: 0.2640, Reconstruction Loss: 0.2620, KL Loss: 0.0020, Test Loss: 0.2615\n",
            "Epoch 9, Train Loss: 0.2639, Reconstruction Loss: 0.2618, KL Loss: 0.0020, Test Loss: 0.2614\n",
            "Epoch 10, Train Loss: 0.2639, Reconstruction Loss: 0.2619, KL Loss: 0.0020, Test Loss: 0.2612\n",
            "Epoch 11, Train Loss: 0.2637, Reconstruction Loss: 0.2617, KL Loss: 0.0020, Test Loss: 0.2611\n",
            "Epoch 12, Train Loss: 0.2636, Reconstruction Loss: 0.2616, KL Loss: 0.0020, Test Loss: 0.2612\n",
            "Epoch 13, Train Loss: 0.2635, Reconstruction Loss: 0.2615, KL Loss: 0.0020, Test Loss: 0.2611\n",
            "Epoch 14, Train Loss: 0.2634, Reconstruction Loss: 0.2613, KL Loss: 0.0021, Test Loss: 0.2610\n",
            "Epoch 15, Train Loss: 0.2634, Reconstruction Loss: 0.2612, KL Loss: 0.0021, Test Loss: 0.2609\n",
            "Epoch 16, Train Loss: 0.2633, Reconstruction Loss: 0.2611, KL Loss: 0.0021, Test Loss: 0.2607\n",
            "Epoch 17, Train Loss: 0.2633, Reconstruction Loss: 0.2611, KL Loss: 0.0022, Test Loss: 0.2607\n",
            "Epoch 18, Train Loss: 0.2632, Reconstruction Loss: 0.2609, KL Loss: 0.0023, Test Loss: 0.2605\n",
            "Epoch 19, Train Loss: 0.2632, Reconstruction Loss: 0.2609, KL Loss: 0.0023, Test Loss: 0.2605\n",
            "Epoch 20, Train Loss: 0.2632, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2608\n",
            "Epoch 21, Train Loss: 0.2632, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2604\n",
            "Epoch 22, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2603\n",
            "Epoch 23, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2604\n",
            "Epoch 24, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2602\n",
            "Epoch 25, Train Loss: 0.2631, Reconstruction Loss: 0.2607, KL Loss: 0.0024, Test Loss: 0.2603\n",
            "Epoch 26, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2603\n",
            "Epoch 27, Train Loss: 0.2629, Reconstruction Loss: 0.2604, KL Loss: 0.0025, Test Loss: 0.2598\n",
            "Epoch 28, Train Loss: 0.2630, Reconstruction Loss: 0.2604, KL Loss: 0.0026, Test Loss: 0.2600\n",
            "Epoch 29, Train Loss: 0.2629, Reconstruction Loss: 0.2603, KL Loss: 0.0026, Test Loss: 0.2600\n",
            "Epoch 30, Train Loss: 0.2631, Reconstruction Loss: 0.2605, KL Loss: 0.0026, Test Loss: 0.2600\n",
            "DecoderEncoderDecoderVAE Final Test Loss: 0.2600\n",
            "\n",
            "Training DoubleEncoderDecoderVAE...\n",
            "Epoch 1, Train Loss: 0.3800, Reconstruction Loss: 0.3779, KL Loss: 0.0022, Test Loss: 0.2727\n",
            "Epoch 2, Train Loss: 0.2700, Reconstruction Loss: 0.2683, KL Loss: 0.0017, Test Loss: 0.2660\n",
            "Epoch 3, Train Loss: 0.2672, Reconstruction Loss: 0.2654, KL Loss: 0.0017, Test Loss: 0.2646\n",
            "Epoch 4, Train Loss: 0.2658, Reconstruction Loss: 0.2640, KL Loss: 0.0019, Test Loss: 0.2632\n",
            "Epoch 5, Train Loss: 0.2650, Reconstruction Loss: 0.2631, KL Loss: 0.0019, Test Loss: 0.2624\n",
            "Epoch 6, Train Loss: 0.2646, Reconstruction Loss: 0.2628, KL Loss: 0.0019, Test Loss: 0.2623\n",
            "Epoch 7, Train Loss: 0.2643, Reconstruction Loss: 0.2623, KL Loss: 0.0020, Test Loss: 0.2617\n",
            "Epoch 8, Train Loss: 0.2640, Reconstruction Loss: 0.2619, KL Loss: 0.0021, Test Loss: 0.2615\n",
            "Epoch 9, Train Loss: 0.2638, Reconstruction Loss: 0.2617, KL Loss: 0.0022, Test Loss: 0.2612\n",
            "Epoch 10, Train Loss: 0.2637, Reconstruction Loss: 0.2616, KL Loss: 0.0022, Test Loss: 0.2613\n",
            "Epoch 11, Train Loss: 0.2636, Reconstruction Loss: 0.2614, KL Loss: 0.0022, Test Loss: 0.2611\n",
            "Epoch 12, Train Loss: 0.2636, Reconstruction Loss: 0.2613, KL Loss: 0.0022, Test Loss: 0.2611\n",
            "Epoch 13, Train Loss: 0.2635, Reconstruction Loss: 0.2613, KL Loss: 0.0022, Test Loss: 0.2609\n",
            "Epoch 14, Train Loss: 0.2635, Reconstruction Loss: 0.2612, KL Loss: 0.0023, Test Loss: 0.2607\n",
            "Epoch 15, Train Loss: 0.2634, Reconstruction Loss: 0.2611, KL Loss: 0.0023, Test Loss: 0.2607\n",
            "Epoch 16, Train Loss: 0.2633, Reconstruction Loss: 0.2609, KL Loss: 0.0024, Test Loss: 0.2603\n",
            "Epoch 17, Train Loss: 0.2632, Reconstruction Loss: 0.2608, KL Loss: 0.0024, Test Loss: 0.2604\n",
            "Epoch 18, Train Loss: 0.2633, Reconstruction Loss: 0.2609, KL Loss: 0.0024, Test Loss: 0.2604\n",
            "Epoch 19, Train Loss: 0.2632, Reconstruction Loss: 0.2608, KL Loss: 0.0024, Test Loss: 0.2606\n",
            "Epoch 20, Train Loss: 0.2632, Reconstruction Loss: 0.2608, KL Loss: 0.0024, Test Loss: 0.2602\n",
            "Epoch 21, Train Loss: 0.2632, Reconstruction Loss: 0.2608, KL Loss: 0.0024, Test Loss: 0.2604\n",
            "Epoch 22, Train Loss: 0.2631, Reconstruction Loss: 0.2607, KL Loss: 0.0024, Test Loss: 0.2604\n",
            "Epoch 23, Train Loss: 0.2632, Reconstruction Loss: 0.2607, KL Loss: 0.0025, Test Loss: 0.2601\n",
            "Epoch 24, Train Loss: 0.2631, Reconstruction Loss: 0.2606, KL Loss: 0.0025, Test Loss: 0.2601\n",
            "Epoch 25, Train Loss: 0.2630, Reconstruction Loss: 0.2604, KL Loss: 0.0026, Test Loss: 0.2598\n",
            "Epoch 26, Train Loss: 0.2630, Reconstruction Loss: 0.2604, KL Loss: 0.0027, Test Loss: 0.2599\n",
            "Epoch 27, Train Loss: 0.2630, Reconstruction Loss: 0.2603, KL Loss: 0.0027, Test Loss: 0.2598\n",
            "Epoch 28, Train Loss: 0.2630, Reconstruction Loss: 0.2603, KL Loss: 0.0027, Test Loss: 0.2599\n",
            "Epoch 29, Train Loss: 0.2629, Reconstruction Loss: 0.2601, KL Loss: 0.0028, Test Loss: 0.2599\n",
            "Epoch 30, Train Loss: 0.2629, Reconstruction Loss: 0.2601, KL Loss: 0.0029, Test Loss: 0.2593\n",
            "DoubleEncoderDecoderVAE Final Test Loss: 0.2593\n",
            "\n",
            "Best performing model: DoubleEncoderDecoderVAE\n"
          ]
        }
      ]
    }
  ]
}